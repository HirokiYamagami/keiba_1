{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各年のレース情報取得\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "#ディレクトリ作成 (好きなディレクトリを指定)\n",
    "os.makedirs(\"./競馬/arima2021/raceInfo/\")\n",
    "\n",
    "#過去20年のレースIDの取得\n",
    "raceid_list = []\n",
    "#URLを変更することで有馬記念以外のレースも取得できるはず\n",
    "url = \"https://db.netkeiba.com/?pid=race_list&word=%5E%CD%AD%C7%CF%B5%AD%C7%B0\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "soup_txt_race = soup.find_all(href = re.compile(\"/race/20\"))\n",
    "for num in range(20):\n",
    "    raceid_list.append(soup_txt_race[num].attrs['href'])\n",
    "\n",
    "#過去20年のレースのデータを取得\n",
    "for count,i in enumerate(raceid_list):\n",
    "    url = \"https://db.netkeiba.com\" + i\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    soup_span = soup.find_all(\"span\")\n",
    "    len(soup_span)\n",
    "\n",
    "    #頭数\n",
    "    allnum = int((len(soup_span) - 8) / 3)\n",
    "\n",
    "    #馬の情報を以下で取得\n",
    "    soup_txt_l = soup.find_all(class_ = \"txt_l\")\n",
    "\n",
    "    #馬の名前\n",
    "    name = []\n",
    "    for num in range(allnum):\n",
    "        name.append(soup_txt_l[4 * num].contents[1].contents[0])\n",
    "\n",
    "    #騎手名\n",
    "    jockey = []\n",
    "    for num in range(allnum):\n",
    "        jockey.append(soup_txt_l[4 * num + 1].contents[1].contents[0])\n",
    "\n",
    "    #馬番\n",
    "    soup_txt_r = soup.find_all(class_ = \"txt_r\")\n",
    "    horse_number = []\n",
    "    for num in range(allnum):\n",
    "        horse_number.append(soup_txt_r[1 + 5 * num].contents[0])\n",
    "\n",
    "    #走破タイム\n",
    "    runtime = [] \n",
    "    for num in range(allnum):\n",
    "        try:\n",
    "            runtime.append(soup_txt_r[2 + num * 5].contents[0])\n",
    "        except IndexError:\n",
    "            runtime.append(None)\n",
    "\n",
    "    #オッズ\n",
    "    odds = []\n",
    "    for num in range(allnum):\n",
    "        try:\n",
    "            odds.append(soup_txt_r[3 + 5 * num].contents[0])\n",
    "        except IndexError:\n",
    "            odds.append(None)\n",
    "\n",
    "    #通過順\n",
    "    soup_nowrap = soup.find_all(\"td\",nowrap = \"nowrap\",class_ = None)\n",
    "    pas = []\n",
    "    for num in range(allnum):\n",
    "        try:\n",
    "            pas.append(soup_nowrap[3 * num].contents[0])\n",
    "        except IndexError:\n",
    "            pas.append(None)\n",
    "\n",
    "    #体重\n",
    "    weight = []\n",
    "    for num in range(allnum):\n",
    "        try:\n",
    "            weight.append(soup_nowrap[3 * num + 1].contents[0])\n",
    "        except IndexError:\n",
    "            weight.append(None)\n",
    "\n",
    "    #性齢\n",
    "    soup_tet_c = soup.find_all(\"td\",nowrap = \"nowrap\",class_  = \"txt_c\")\n",
    "    sex_old = []\n",
    "    for num in range(allnum):\n",
    "        sex_old.append(soup_tet_c[6 * num].contents[0])\n",
    "\n",
    "    #斤量\n",
    "    handi = []\n",
    "    for num in range(allnum):\n",
    "        handi.append(soup_tet_c[6 * num + 1].contents[0])\n",
    "\n",
    "    #上がり\n",
    "    last = []\n",
    "    for num in range(allnum):\n",
    "        try:\n",
    "            last.append(soup_tet_c[6 * num + 3].contents[0].contents[0])\n",
    "        except IndexError:\n",
    "            last.append(None)\n",
    "\n",
    "    #人気\n",
    "    pop = []\n",
    "    for num in range(allnum):\n",
    "        try:\n",
    "            pop.append(soup_span[3 * num + 10].contents[0])\n",
    "        except IndexError:\n",
    "            pop.append(None)\n",
    "    \n",
    "    #データ格納\n",
    "    houseInfo = [name,jockey,horse_number,runtime,odds,pas,weight,sex_old,handi,last,pop]\n",
    "    \n",
    "    #CSV書き出し\n",
    "    #ファイルパス指定\n",
    "    year = 2021-count    \n",
    "    filepass1 = \"./競馬/arima2021/raceInfo/arima{}_test.csv\"\n",
    "    filepass2 = filepass1.format(year)\n",
    "    with open(filepass2, 'a', newline = '',encoding = \"SHIFT-JIS\") as f:\n",
    "        csv.writer(f).writerows(houseInfo)\n",
    "    col_num = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    df = pandas.read_csv(filepass2,encoding = \"SHIFT-JIS\",names = col_num)\n",
    "    \n",
    "    #列名追加\n",
    "    df_mod = df.rename(index = {0:\"馬名\",1:\"騎手名\",2:\"枠順\",3:\"走破タイム\",4:\"オッズ\",5:\"通過順位\",6:\"馬体重\",7:\"性齢\",8:\"斤量\",9:\"上がり3ハロン\",10:\"人気\"})\n",
    "  \n",
    "    #ファイル書き出し\n",
    "    filepass3 = \"./競馬/arima2021/raceInfo/arima{}.csv\"\n",
    "    filepass4 = filepass3.format(year)\n",
    "    df_mod.to_csv(filepass4)\n",
    "    \n",
    "    #testファイルの削除\n",
    "    os.remove(filepass2)\n",
    "    \n",
    "    #間隔\n",
    "    time.sleep(1)\n",
    "\n",
    "#各年の出走馬のデータ取得\n",
    "\n",
    "#関数定義\n",
    "#列名から部分一致でindexを返す関数の定義\n",
    "def inclusive_index(lst, purpose):\n",
    "    for i, e in enumerate(lst):\n",
    "        if purpose in e: return i\n",
    "\n",
    "    raise IndexError\n",
    "\n",
    "#ディレクトリ作成\n",
    "os.makedirs(\"./競馬/arima2021/horseInfo/\")\n",
    "\n",
    "#過去20年のレースIDの取得\n",
    "baseurl = \"https://db.netkeiba.com/?pid=race_list&word=%5E%CD%AD%C7%CF%B5%AD%C7%B0\"\n",
    "r1= requests.get(baseurl)\n",
    "soup1 = BeautifulSoup(r1.content, \"html.parser\")\n",
    "soup1_txt_race = soup1.find_all(href = re.compile(\"/race/20\"))\n",
    "\n",
    "#レースidと検索用パラメータの格納\n",
    "race_para_list=[]\n",
    "raceid_list = []\n",
    "for num in range(20):\n",
    "    race_para_list.append(soup1_txt_race[num])\n",
    "    raceid_list.append(soup1_txt_race[num].attrs['href'])\n",
    "\n",
    "#過去20年のレースのデータを取得\n",
    "for count,i in enumerate(raceid_list):\n",
    "    raceurl = \"https://db.netkeiba.com\" + i\n",
    "    r2 = requests.get(raceurl)\n",
    "    soup2 = BeautifulSoup(r2.content, \"html.parser\")\n",
    "    soup2_span = soup2.find_all(\"span\")\n",
    "    len(soup2_span)\n",
    "    #出走頭数\n",
    "    allnum = int((len(soup2_span) - 8) / 3)\n",
    "\n",
    "    #出走馬のページURL取得\n",
    "    soup2_txt_h = soup2.find_all(href = re.compile(\"/horse/\"))\n",
    "    #出走馬のURL\n",
    "    horse_url_list = [] \n",
    "    for num in range(allnum):\n",
    "        horse_url_list.append(soup2_txt_h[num].attrs['href'])\n",
    "        \n",
    "    #出走馬のデータ取得\n",
    "    for horse_url in horse_url_list:\n",
    "        horseurl = \"https://db.netkeiba.com\"+horse_url\n",
    "        r3 = requests.get(horseurl)\n",
    "        soup3 = BeautifulSoup(r3.content, \"html.parser\")\n",
    "\n",
    "        # 直近3回の出走レース名の取得\n",
    "        soup3_txt_race = soup3.find_all(href = re.compile(\"/race/20\"))\n",
    "        soup3_txt_race_str = [str(n) for n in soup3_txt_race]\n",
    "        idx=soup3_txt_race_str.index(str(race_para_list[count]))\n",
    "        recent_race=[soup3_txt_race[idx+1],soup3_txt_race[idx+2],soup3_txt_race[idx+3]]\n",
    "        recent_race_str=[str(n) for n in recent_race] \n",
    "\n",
    "        recent_race_list = []\n",
    "        for num2 in range(0,3):\n",
    "            try:\n",
    "                recent_race_list.append(recent_race[num2].contents[0])\n",
    "            except IndexError:\n",
    "                recent_race_list.append(None)\n",
    "\n",
    "        #直近3回の出走レースの詳細情報の取得 \n",
    "        soup3_td = soup3.find_all('td')\n",
    "        soup3_td_str = [str(n) for n in soup3_td]\n",
    "\n",
    "        recent_race_info = []\n",
    "        for race in  recent_race_str:\n",
    "            try:\n",
    "                idx2=inclusive_index(soup3_td_str,race)\n",
    "                recent_race_info.append(race)\n",
    "                recent_race_info.append(soup3_td[idx2+5].text)\n",
    "                #recent_race_info.append(soup3_td[idx2+6].text)\n",
    "                recent_race_info.append(soup3_td[idx2+7].text)\n",
    "            except IndexError:\n",
    "                recent_race_info.append(None)\n",
    "       #print(recent_race_info)\n",
    "            \n",
    "        #みんなの評価の取得\n",
    "        soup_txt_review = soup3.find_all(src = re.compile(\"https://cdn.netkeiba.com/img.db//style/netkeiba.ja/image/review_bar_\"))\n",
    "        #芝適正(値が大きいほどダート適正)\n",
    "        turf_type = soup_txt_review[1].get(\"width\")\n",
    "        #距離適性(値が大きいほど長距離適性)\n",
    "        dist_type = soup_txt_review[3].get(\"width\")\n",
    "        #脚質(値が大きいほど追い込み)\n",
    "        run_type = soup_txt_review[5].get(\"width\")\n",
    "        #成長(値が大きいほど晩成)\n",
    "        grow_type = soup_txt_review[7].get(\"width\")\n",
    "        #馬場適性(値が大きいほど重馬場苦手)\n",
    "        field_type = soup_txt_review[9].get(\"width\")\n",
    "\n",
    "        #馬の総合評価\n",
    "        soup_stars = soup3.find_all(class_ = re.compile(\"star\"))\n",
    "        #総合評価\n",
    "        try:\n",
    "            soup_all_stars = soup_stars[0].contents[0].contents[0]\n",
    "        except IndexError:\n",
    "            soup_all_stars = \"\"\n",
    "\n",
    "        #実績評価\n",
    "        try:\n",
    "            soup_result_stars = soup_stars[1].contents[0]\n",
    "        except IndexError:\n",
    "            soup_result_stars = \"\"\n",
    "        #ポテンシャル評価\n",
    "        try:\n",
    "            soup_potential_stars = soup_stars[2].contents[0].contents[0]\n",
    "        except IndexError:\n",
    "            soup_potential_stars = \"\"\n",
    "            \n",
    "            \n",
    "        #全ての結果を結合\n",
    "        eachhorseInfo = [turf_type,dist_type,run_type,grow_type,field_type,soup_all_stars,soup_result_stars,soup_potential_stars]+recent_race_info\n",
    "        #不正な文字コードを削除\n",
    "        eachhorseInfo_mod = []\n",
    "        for item in eachhorseInfo:\n",
    "            item_mod = item.replace(\"\\xa0\",\"\") \n",
    "            eachhorseInfo_mod.append(item_mod)\n",
    "            \n",
    "        #CSVに書き出し\n",
    "        year = 2021-count\n",
    "        filepass1 = \"./競馬/arima2021/horseInfo/arimahorse{}_test.csv\"\n",
    "        filepass2 = filepass1.format(year)\n",
    "        with open(filepass2, 'a',newline = '',encoding = \"SHIFT-JIS\") as f:\n",
    "            csv.writer(f).writerow(eachhorseInfo_mod)\n",
    "    \n",
    "#csvの整理\n",
    "    col_names=[\"芝適性\",\"距離適性\",\"脚質\",\"成長\",\"馬場適性\",\"総合評価\",\"実績評価\",\"ポテンシャル評価\",\"前走レース名\",\"前走オッズ\",\"前走成績\",\"2走前レース名\",\"2走前オッズ\",\"2走前成績\",\"3走前レース名\",\"3走前オッズ\",\"3走前成績\"]\n",
    "    df = pandas.read_csv(filepass2,encoding = \"SHIFT-JIS\",names=col_names)\n",
    "    year = 2021-count\n",
    "    filepass3 = \"./競馬/arima2021/horseInfo/arimahorse{}.csv\"\n",
    "    filepass4 = filepass3.format(year)\n",
    "    #転置\n",
    "    df.T.to_csv(filepass4)\n",
    "    #testファイルの削除\n",
    "    os.remove(filepass2)\n",
    "    \n",
    "\n",
    "#レースデータと出走馬のデータの結合\n",
    "\n",
    "#ディレクトリ作成\n",
    "os.makedirs(\"./競馬/arima2021/allInfo/\")\n",
    "\n",
    "for year in range(2002,2022):\n",
    "    racepass = \"./競馬/arima2021/raceInfo/arima\" + str(year) + \".csv\"\n",
    "    horsepass = \"./競馬/arima2021/horseInfo/arimahorse\" + str(year) + \".csv\"\n",
    "    df1 = pandas.read_csv(racepass)\n",
    "    df2 = pandas.read_csv(horsepass)\n",
    "    df_concat = pandas.concat([df1,df2], axis = 0, ignore_index = False)\n",
    "    allInfopass_tmp = \"./競馬/arima2021/allInfo/arima{}.csv\"\n",
    "    allInfopass = allInfopass_tmp.format(year)\n",
    "    df_concat.to_csv(allInfopass, index = False)\n",
    "\n",
    "# テスト\n",
    "pandas.read_csv(\"/Users/yamagamihiroki/競馬　ラップ分析/keiba/競馬/arima2021/allInfo/arima2020.csv\",index_col=0,encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
